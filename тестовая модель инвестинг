import urllib.request
from bs4 import BeautifulSoup

from transformers import AutoModelForSequenceClassification
from transformers import TFAutoModelForSequenceClassification
from transformers import AutoTokenizer, AutoConfig
import numpy as np
from scipy.special import softmax
# Preprocess text (username and link placeholders)
def preprocess(text):
    new_text = []
    for t in text.split(" "):
        t = '@user' if t.startswith('@') and len(t) > 1 else t
        t = 'http' if t.startswith('http') else t
        new_text.append(t)
    return " ".join(new_text)
MODEL = f"cardiffnlp/twitter-roberta-base-sentiment-latest"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
config = AutoConfig.from_pretrained(MODEL)

model = AutoModelForSequenceClassification.from_pretrained(MODEL)
#model.save_pretrained(MODEL)
text = "Covid cases are increasing fast!"
text = preprocess(text)
encoded_input = tokenizer(text, return_tensors='pt')
output = model(**encoded_input)
scores = output[0][0].detach().numpy()
scores = softmax(scores)
# # TF
# model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)
# model.save_pretrained(MODEL)
# text = "Covid cases are increasing fast!"
# encoded_input = tokenizer(text, return_tensors='tf')
# output = model(encoded_input)
# scores = output[0][0].numpy()
# scores = softmax(scores)
# Print labels and scores
ranking = np.argsort(scores)
ranking = ranking[::-1]
for i in range(scores.shape[0]):
    l = config.id2label[ranking[i]]
    s = scores[ranking[i]]
    print(f"{i+1}) {l} {np.round(float(s), 4)}")

def fetch_html(url):
    try:
        response = urllib.request.urlopen(url)
        html = response.read().decode('utf-8')
        return html
    except urllib.error.URLError as e:
        print(f"Ошибка при выполнении запроса: {e.reason}")
        return None

# Получение HTML-кода
url = "https://www.zerohedge.com/commodities"
html_content = fetch_html(url)
lst = []
if html_content:
    # Создание объекта BeautifulSoup
    soup = BeautifulSoup(html_content, 'html.parser')
    # Пример: Извлечение всех ссылок из HTML
    links = soup.find_all('a')
    for link in links:
        #print(link.get('href'))
        if 'commodities' in str(link.get('href')) and '#com' not in str(link.get('href')):
          lst.append('https://www.zerohedge.com' + str(link.get('href')))
    print(len(lst))
    # Пример: Извлечение определенных элементов по классу или id
    title_element = soup.find('h1', class_='post-title')
    #if title_element:
        #print(title_element.text)
from bs4 import BeautifulSoup
import urllib.request
for element in lst:
 try:
 #if element == 'https://www.zerohedge.com/commodities/californias-cannabis-market-crashes-northern-counties-brace-impact':
    # Загрузка HTML страницы
    url_each_lst = element
    response = urllib.request.urlopen(url_each_lst)
    html_content = response.read()

    # Создание объекта BeautifulSoup для парсинга HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # Нахождение всех параграфов на странице
    paragraphs = soup.find_all('p')
    print(element)
    text_element = []
    # Вывод содержимого каждого параграфа
    for paragraph in paragraphs:
        text_element.append(paragraph.text)
    MODEL = f"cardiffnlp/twitter-roberta-base-sentiment-latest"
    tokenizer = AutoTokenizer.from_pretrained(MODEL)
    config = AutoConfig.from_pretrained(MODEL)
    # PT
    model = AutoModelForSequenceClassification.from_pretrained(MODEL)
    # model.save_pretrained(MODEL)
    for sum_par in text_element:
       text = sum_par
    text = preprocess(text)
    encoded_input = tokenizer(text, return_tensors='pt')
    output = model(**encoded_input)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    ranking = np.argsort(scores)
    ranking = ranking[::-1]

    for i in range(scores.shape[0]):
        l = config.id2label[ranking[i]]
        s = scores[ranking[i]]

        print(f"{i + 1}) {l} {np.round(float(s), 4)}")
 except:
     print('ссылка не та')
print('Программа завершена')

