import torch
import urllib.request
from bs4 import BeautifulSoup
import urllib.request
from bs4 import BeautifulSoup
from transformers import AutoModelForSequenceClassification
from transformers import TFAutoModelForSequenceClassification
from transformers import AutoTokenizer, AutoConfig
import numpy as np
from scipy.special import softmax
from bs4 import BeautifulSoup
import urllib.request

from transformers import logging
logging.set_verbosity_error()

MODEL = f"cardiffnlp/twitter-roberta-base-sentiment-latest"
tokenizer = AutoTokenizer.from_pretrained(MODEL)
config = AutoConfig.from_pretrained(MODEL)
url = 'https://dzen.ru/news?issue_tld=ru&utm_referer=yandex.ru'#"https://www.zerohedge.com/commodities"
url_set = set()#создаем список,куда будут добавлены ссылки
test_list = []

def model_proccesing(text,MODEL):
    model = AutoModelForSequenceClassification.from_pretrained(MODEL)
    encoded_input = tokenizer(text, return_tensors='pt')
    output = model(**encoded_input)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)

    ranking = np.argsort(scores)
    ranking = ranking[::-1]
    for i in range(scores.shape[0]):
        l = config.id2label[ranking[i]]
        s = scores[ranking[i]]
        print(f"{i+1}) {l} {np.round(float(s), 4)}")


def fetch_html(url):
    try:
        response = urllib.request.urlopen(url)
        html = response.read().decode('utf-8')
        return html
    except urllib.error.URLError as e:
        print(f"Ошибка при выполнении запроса: {e.reason}")
        return None
# Получение HTML-кода
html_content = fetch_html(url)
if html_content:
    # Создание объекта BeautifulSoup
    soup = BeautifulSoup(html_content, 'html.parser')
    # Пример: Извлечение всех ссылок из HTML
    links = soup.find_all('a')
    for link in links:
        #print(link.get('href'))
        test_list.append(link.get('href'))
        if 'commodities' in str(link.get('href')) and '#com' not in str(link.get('href')):
          url_set.add('https://www.zerohedge.com' + str(link.get('href')))
    print(len(url_set))
    # Пример: Извлечение определенных элементов по классу или id
    title_element = soup.find('h1', class_='post-title')
    #if title_element:
        #print(title_element.text)

for element in url_set:
    # Загрузка HTML страницы
    url_each_lst = element
    response = urllib.request.urlopen(url_each_lst)
    html_content = response.read()

    # Создание объекта BeautifulSoup для парсинга HTML
    soup = BeautifulSoup(html_content, 'html.parser')

    # Нахождение всех параграфов на странице
    paragraphs = soup.find_all('p')
    print(element)
    text_element = []
    # Вывод содержимого каждого параграфа
    for paragraph in paragraphs:
        text_element.append(paragraph.text)
    MODEL = f"cardiffnlp/twitter-roberta-base-sentiment-latest"
    tokenizer = AutoTokenizer.from_pretrained(MODEL)
    config = AutoConfig.from_pretrained(MODEL)
    # PT
    model = AutoModelForSequenceClassification.from_pretrained(MODEL)
    # model.save_pretrained(MODEL)
    for sum_par in text_element:
       text = sum_par
    text = element
    encoded_input = tokenizer(text, return_tensors='pt')
    output = model(**encoded_input)
    scores = output[0][0].detach().numpy()
    scores = softmax(scores)
    ranking = np.argsort(scores)
    ranking = ranking[::-1]

    for i in range(scores.shape[0]):
        l = config.id2label[ranking[i]]
        s = scores[ranking[i]]

        print(f"{i + 1}) {l} {np.round(float(s), 4)}")
print('Программа завершена')

